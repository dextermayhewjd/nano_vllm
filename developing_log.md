å¼€å‘æ—¥å¿—

2025-12-9

- ä»Šæ—¥å¤„ç†wslå’Œshadowsocket

  1. è®°å¾—å¼€allow other device to connect

  2. ä½†æ˜¯åŒæ—¶ä¹Ÿè¦è®°å¾—æŠŠé˜²ç«å¢™æ‰“å¼€ï¼ˆåœ¨ä¸ä¸‹è½½çš„æ—¶å€™ï¼‰

  3. ä»£ç†éƒ¨åˆ†è¦è€ƒè™‘è°ƒå›

  4. å…¶æ¬¡éœ€è¦æ¡python interpreter

      4.1éœ€è¦vscodeæ”¯æŒ wslç‰ˆæœ¬çš„
  
      4.2éœ€è¦ç¡®ä¿èƒ½å¤Ÿpipèƒ½å¤Ÿä½¿ç”¨ è¦è£…ä¸€ä¸ªé¢å¤–åŒ…æ‰èƒ½ç”¨pysockå¥½åƒ
  
      4.3åå¤éªŒè¯æ˜¯å¦èƒ½å¤Ÿ

  - # tokenizer 
  ä½¿ç”¨çš„æ˜¯huggingfaceçš„autokenizer 
  ```python
    from transformers import AutoTokenizer

  # Download vocabulary from huggingface.co and cache.
  tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")

  # Download vocabulary from huggingface.co (user-uploaded) and cache.
  tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

  # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
  # tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")

  # Download vocabulary from huggingface.co and define model-specific arguments
  tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-base", add_prefix_space=True)

  # Explicitly use the tokenizers backend
  tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer", backend="tokenizers")

  # Explicitly use the sentencepiece backend
  tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer", backend="sentencepiece")
  ```

  ç„¶åå¯¹äºtokenizerçš„encodeå’Œdecode åœ¨è¿™é‡Œ  
  huggingfaceçš„encodeå’Œdecodeæ€ä¹ˆç”¨å¯ä»¥çœ‹[[https://huggingface.co/docs/transformers/v5.0.0rc0/zh/main_classes/tokenizer#transformers.TokenizersBackend.encode]]


  # decodeå’Œ encode 
  encode åŠ äº† return tensors="pt" ï¼ˆpytorch tensorï¼‰  
  å› ä¸ºè¦è¿”å›2dçš„tensorï¼ˆbatchï¼‰ä¸º1  
  1. ä¸åŠ   
  [15492,123] (python list[int])  

  2. åŠ äº†  
  tensor([[15496,  995]])  
  è¿”å› äºŒç»´ tensorï¼ˆshape = [1, seq_len]ï¼‰  
  å› ä¸ºencodeä¸€èˆ¬æ˜¯batch encode  
  ä½†æˆ‘åªæ˜¯ä¸€ä¸ªprompt æ‰€ä»¥é»˜è®¤å–ç”¨[0]  
  ```python
        # out["input_ids"] shape: (1, seq_len) 
        # è¦å˜æˆâ†’ return 1D 
        
        # return out["input_ids"] 
        # #test Token IDs: tensor([[108386,   3837,  99489]])
        
        return out["input_ids"][0]
        #Token IDs: tensor([108386,   3837,  99489])
  ```



  å¦‚æœç›´æ¥ä½¿ç”¨tokenizeræœ¬èº« é‚£ä¹ˆçœ‹__call__ï¼ˆè¯­æ³•ç³–ï¼‰  

    return_tensors (str or TensorType, optional) â€”  
    If set, will return tensors    
    instead of list of python integers. Acceptable values are:  
    'pt': Return PyTorch torch.Tensor objects.  
    'np': Return Numpy np.ndarray objects.  
    è¿™é‡Œå¯ä»¥çœ‹å‡ºå¯ä»¥è¿”å›pt ç„¶ååç»­returnç”Ÿæˆçš„é‡Œé¢åŒ…å«çš„ä¸€é¡¹æ˜¯tensor  
return typeæ˜¯BatchEncoding
[[https://huggingface.co/docs/transformers/v5.0.0rc0/zh/main_classes/tokenizer#transformers.TokenizersBackend.__call__.return_tensors]]

## ä¸‹è½½qwen2-7b 
```bash
pip install "huggingface_hub[cli]"
```
ä¸‹è½½åˆ°æŒ‡å®šç›®å½•ï¼š
```bash
huggingface-cli download \
    qwen/Qwen2-7B \
    --local-dir /home/yourname/models/Qwen2-7B
```
è‡ªå·±ç”¨è‚¯å®šæ˜¯è¦æ¢åå­—å•¦
```bash
huggingface-cli download \
    qwen/Qwen2-7B \
    --local-dir /home/dexterding/models/Qwen2-7B
```

# 2025-12-10
## å¼€å‘è¿‡ç¨‹ä¸­å†™å°testæ¥æ˜ç™½åœ¨å¹²å˜›

å†™äº†ä¸€ä¸ªå°test ç”¨æ¥äº†è§£æ¯ä¸€æ­¥tokenizeråœ¨å¹²ä»€ä¹ˆ  
åœ¨/run_test/test_tokenizer_qwen2.py ä¸­  

```bash
python run_test/test_tokenizer_qwen2.py
```

```python
from transformers import AutoTokenizer
import torch
class Tokenizer:
    def __init__(self, model_name:str):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        
    def encode(
        self, 
        prompt: str,
        add_special_tokens:bool = True
        )-> torch.Tensor: # 1D tensor: (seq_len,)
      #__call__è¯­æ³•ç³– 
        out = self.tokenizer(
            prompt,
            add_special_tokens=add_special_tokens,
            return_tensors="pt",
        ) # æ­¤å¤„è¿”å›çš„æ˜¯batchEncoding 
      
        # out["input_ids"] shape: (1, seq_len) 
        # è¦å˜æˆâ†’ return 1D 
        
        # return out["input_ids"] 
        # #test Token IDs: tensor([[108386,   3837,  99489]])
        
        return out["input_ids"][0]
        #Token IDs: tensor([108386,   3837,  99489])
    
    def decode(self,
               token_ids,
               skip_special_tokens:bool = True):
      return self.tokenizer.decode(token_ids,skip_special_tokens = skip_special_tokens)
    
```
å¯ä»¥çœ‹å‡ºå‡ ç‚¹  
ç¬¬ä¸€éœ€è¦æ—¶çš„æ—¶å€™éœ€è¦å…ˆå®šä¹‰ model_path æœ€å¥½é¿å…ç¡¬ç¼–ç   
ç¬¬äºŒåœ¨ä½¿ç”¨è‡ªå·±å†™çš„ä¸œè¥¿çš„æ—¶å€™ ä¸åŒæ–‡ä»¶å¤¹ä¸‹é¢æ˜¯from æ–‡ä»¶å¤¹.pythonæ–‡ä»¶å import å…·ä½“class  
ç¬¬ä¸‰ä½¿ç”¨çš„æ—¶å€™æ‰“å°

## é¿å…ç¡¬ç¼–ç 

ä¸ºäº†ä»¥åé¿å…ç¡¬ç¼–ç è·¯å¾„ï¼Œå¯ä»¥å†™ï¼š

configs/model_paths.yaml

```yaml
qwen2_7b: "/home/dexterding/models/Qwen2-7B"
```


ç„¶åå†™ä¸€ä¸ª loaderï¼š 
åœ¨engine/config_loader.py  
è¾“å…¥è¦çš„æ¨¡å‹åå­— 
ä¼šè¿”å›ä½ç½® 
å­˜æ”¾ä½ç½®çš„åœ°æ–¹åœ¨configçš„model_paths.yamlé‡Œ


è¿™æ ·å¯ä»¥éšæ—¶å¯ä»¥åˆ‡æ¢æ¨¡å‹ï¼Œä¸å½±å“æµ‹è¯•ä»£ç ã€‚ 


## Model Loader æ¨¡å‹åŠ è½½å™¨

### å’Œengineçš„åŒºåˆ†
1. è¿™é‡Œåªæ‹¿æ¨¡å‹æœ¬èº«
2. åˆ©ç”¨config loaderåœ¨configé‡Œyamlé‡Œæ‹¿å®šä¹‰çš„æ¨¡å‹ï¼ˆå­˜åœ¨wslé‡Œçš„ï¼‰çš„path
3. æ­¤å¤„è°ƒç”¨huggingfaceçš„æ¥å£ AutoCausalLM ç”¨æ¥ç›´æ¥å¾—åˆ°å¯¹åº”çš„æ¨¡å‹æ¶æ„ï¼ˆä¸ç”¨è‡ªå·±æ‰‹å†™ï¼‰
3.1 model_path å¿…é¡»æä¾› å¦‚æœæä¾›çš„æ˜¯åå­— é‚£ä¹ˆä¼šå»ä¸‹è½½
3.2 éœ€è¦é¢å¤–ç¦æ­¢ä¸€ä¸‹localfile only
3.3 è¿™é‡Œç»ƒä¹ åŸºç¡€ ä¸ç›´æ¥ä½¿ç”¨huggingfaceçš„è‡ªåŠ¨æŒ¡ ä¾‹å¦‚æŠŠæ¨¡å‹æ”¾åœ¨å“ªå¼ æ˜¾å¡å•Šæ”¾å†…å­˜å•Šä¹‹ç±»çš„ ï¼ˆæœ¬è´¨æ˜¯torchçš„è¯­æ³•ï¼‰
3.4 å¼€å¯evalæ¨¡å¼


# ModelLoader çš„ä½œç”¨ä¸ Engine çš„åŒºåˆ†

### 1. ModelLoader çš„èŒè´£æ˜¯â€œåŠ è½½æ¨¡å‹æœ¬ä½“â€

- ä»…è´Ÿè´£ä»ç£ç›˜/WsL åŠ è½½æ¨¡å‹æƒé‡ä¸æ¶æ„ã€‚
- ä¸è´Ÿè´£æ¨ç†æµç¨‹ã€ä¸è´Ÿè´£ç”Ÿæˆé€»è¾‘ã€ä¸è´Ÿè´£ KV cacheï¼Œä¸è´Ÿè´£ tokenizerã€‚

- æ‰€æœ‰â€œè¿è¡Œæ—¶è¡Œä¸ºï¼ˆå¦‚ç”Ÿæˆ token æµã€çŠ¶æ€ç»´æŠ¤ï¼‰â€äº¤ç»™ Engineã€‚

### 2. ä»é…ç½®ç³»ç»Ÿä¸­è¯»å–æ¨¡å‹è·¯å¾„

- é€šè¿‡ config loader ä» configs/model_paths.yaml ä¸­è§£ææ¨¡å‹è·¯å¾„ã€‚

- path æŒ‡å‘ WSL æœ¬åœ°ç›®å½•ï¼šä¾‹å¦‚ /home/dexterding/models/Qwen2-7B

- è¿™æ ·é¿å…åœ¨ä»£ç ä¸­å†™æ­»è·¯å¾„ï¼Œä½¿æ¨¡å‹åˆ‡æ¢æ›´çµæ´»ã€‚

### 3. åˆ©ç”¨ HuggingFace AutoModelForCausalLM è·å–æ¨¡å‹æ¶æ„

- é€šè¿‡ AutoModelForCausalLM.from_pretrained(model_path) è‡ªåŠ¨æ„å»º Qwen2 çš„ç½‘ç»œç»“æ„ã€‚

- ä¸éœ€è¦æ‰‹åŠ¨ç¼–å†™ PyTorch çš„ transformer å±‚ï¼Œå®ç°â€œæ¡†æ¶å³æ¶æ„â€ã€‚

- æ¨¡å‹ç±»åŸºäº config è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„å®ç°ï¼ˆå¦‚ Qwen2ForCausalLMï¼‰ã€‚

### 4. æ¨¡å‹è·¯å¾„è®¾å®šç›¸å…³ç»†èŠ‚

- å¿…é¡»æä¾› model_pathï¼Œå¦åˆ™ä¼šé»˜è®¤ä¸‹è½½ç½‘ç»œæ¨¡å‹ã€‚

- å¼€å‘é˜¶æ®µåº”å¼ºåˆ¶ï¼š
```python
local_files_only=True
```
  é¿å…è¯¯è§¦å‘åœ¨çº¿ä¸‹è½½è¡Œä¸ºã€‚

### 5.ä¸ä½¿ç”¨ HF çš„â€œè‡ªåŠ¨åˆ†å¸ƒå¼/è‡ªåŠ¨ placementâ€èƒ½åŠ›ï¼ˆåˆ»æ„è®­ç»ƒåŸºç¡€èƒ½åŠ›ï¼‰

- ä¸ä½¿ç”¨ Accelerate

- ä¸ä½¿ç”¨ device_map="auto"
â†’ ç›®çš„æ˜¯ç»ƒä¹ åº•å±‚ PyTorch çš„æ¨¡å‹æ”¾ç½®è¯­æ³•ï¼š

- model.to("cuda:0")
 å¹¶ç†è§£æ˜¾å­˜ç®¡ç†ï¼Œè€Œä¸æ˜¯è¢«è‡ªåŠ¨æŒ¡æŠ½è±¡é®è”½ã€‚

### 6. å¼€å¯ eval() æ¨¡å¼

ç¦æ­¢æ¢¯åº¦ï¼šèŠ‚çº¦æ˜¾å­˜å¹¶é˜²æ­¢è¯¯åå‘ã€‚

æ¨ç†æ¨¡å¼ä¸‹ LayerNormã€Dropout ä¼šåˆ‡æ¢åˆ° inference è¡Œä¸ºã€‚



å®Œå–„äº†MVP
LLM é€‰ç”¨çš„æ˜¯ qwen2-7b ä½¿ç”¨å•å¡24gbå¯è¿è¡Œ
model loaderå’Œtokenizer loaderè¾“å…¥model_nameåè‡ªåŠ¨è°ƒç”¨config loaderçš„load model path
æ³¨æ„çš„ç‚¹1
tokenizerçš„initæ˜¯ä½¿ç”¨model name
modelLoader initä¹Ÿæ˜¯model name
ä½†æ˜¯å¯¹åº”çš„AutoTokenizer.from_pretrained
å’Œ AutoModelForCausalLM.from_pretrained
éƒ½æ˜¯éœ€è¦çš„model path å¹¶ä¸”æ˜¯è®¿é—®æœ¬åœ°local file
local fileå­˜æ”¾åœ¨configçš„yamlä¸­
æ³¨æ„çš„ç‚¹2 ç›®å‰çš„tokenizer å®ç°çš„æ˜¯
è¾“å‡º2d tensor ï¼ˆ1,seq_lenï¼‰ è½¬1d tensorï¼ˆseq_lenï¼‰
å› ä¸ºåªæœ‰ä¸€ä¸ªprompt ä½†æ˜¯tokenizeré»˜è®¤çš„æ˜¯batch_size
ç›®å‰å®ç°çš„mvpå¯¹åº”çš„å•ä¸ªpromptä¸”ä¸€æ¬¡ä¸€ä¸ªtoken è¾“å‡º
æ³¨æ„çš„ç‚¹3
æ¨¡å‹è¿è¡Œçš„device æ˜¯å–å†³äº modeLoaderçš„deviceçš„
ä¸æ˜¯å¾ˆå¥½åº”è¯¥ä¸‹ä¸ªç‰ˆæœ¬æ”¹
åœ¨simpleengineé‡Œä¹Ÿæ˜¯ä»model_loaderçš„deviceé‡Œè·å–çš„device
æ³¨æ„çš„ç‚¹4 
ç›®å‰å› ä¸ºmodel éœ€è¦çš„æ˜¯batch sizeçš„input æ‰€ä»¥å•ä¸ªçš„promptè½¬æ¢æˆçš„id
éœ€è¦ä»ï¼ˆsequenceï¼‰ è½¬åŒ–ä¸ºï¼ˆ1ï¼Œsequenceï¼‰
outputsé‡Œå­˜çš„ é™¤äº†logitä¹‹å¤–å› ä¸ºæœ‰use_cache æ‰€ä»¥ä¼šè¿”å›cache
prefilçš„modelæ˜¯è¾“å…¥prompt+idså’Œuse_cache
ä½†æ˜¯ä¹‹åçš„ä¸€ä¸ªä¸€ä¸ªç®—tokençš„éƒ¨åˆ† 
ä½¿ç”¨çš„æ˜¯
1.å•ä¸ªæœ€æ–°ç”Ÿæˆçš„tokenè½¬åŒ–ä¸ºtensorå“¦
2. ä»¥åŠè¿‡å»çš„past key values
3.ä¸€æ¬¡æ¬¡è¦†ç›–for loopä¹‹å‰çš„ 
modelå…¶å®æ˜¯ä¸¤ç§ç®—æ³•çŠ¶æ€ prefillå’Œdecodeæ˜¯ä¸¤ä¸ª
æœ€åä½¿ç”¨tokenizerçš„decode


### 2025-12-11 å¼€å‘æ—¥å¿—ï½œMVP æ¢³ç†

**1. MVP çŠ¶æ€**

- å®Œæˆæœ€å°å¯è¿è¡Œç‰ˆæœ¬ï¼ˆMVPï¼‰
- LLM é€‰ç”¨ **qwen2-7b**ï¼Œåœ¨å•å¡ **24GB** æ˜¾å­˜ä¸Šå¯æ­£å¸¸è¿è¡Œ

---

**2. Model / Tokenizer / Config å…³ç³»**

- `ModelLoader` å’Œ `Tokenizer` çš„åˆå§‹åŒ–å‚æ•°éƒ½æ˜¯ `model_name`
- å†…éƒ¨ä¼šé€šè¿‡ `ConfigLoader`ï¼š
  - æ ¹æ® `model_name` è¯»å–å¯¹åº”çš„ **model path**ï¼ˆæœ¬åœ°è·¯å¾„ï¼‰ï¼Œé…ç½®å­˜æ”¾åœ¨ `config/*.yaml` ä¸­  
  - `AutoTokenizer.from_pretrained(...)` å’Œ `AutoModelForCausalLM.from_pretrained(...)` å®é™…ä½¿ç”¨çš„æ˜¯ **æœ¬åœ° model path**ï¼Œåªè®¿é—® local fileï¼Œä¸ä¾èµ–åœ¨çº¿ä¸‹è½½

---

**3. Tokenizer å®ç°ç»†èŠ‚**

- `Tokenizer.__init__` åªæ¥æ”¶ **model name**ï¼Œä¸ç›´æ¥æ¥è§¦è·¯å¾„
- å½“å‰ `encode` çš„è¡Œä¸ºï¼š
  - HuggingFace é»˜è®¤è¿”å› **2D tensor**ï¼š`(1, seq_len)`ï¼ˆå› ä¸ºé»˜è®¤æœ‰ batch ç»´åº¦ï¼‰
  - ç”±äºå½“å‰ä»…æ”¯æŒå•ä¸ª promptï¼Œå½“å‰å–`[0]`æŠŠè¾“å‡ºå‹æˆ **1D tensor**ï¼š`(seq_len,)`
- å½“å‰ MVP å‡è®¾ï¼š
  - **åªå¤„ç†å•ä¸ª prompt**
  - **ä¸€æ¬¡åªç”Ÿæˆä¸€ä¸ª token** çš„è‡ªå›å½’è¾“å‡ºé€»è¾‘

---

**4. Device ç®¡ç†ï¼ˆæœ‰å¾…æ”¹è¿›ï¼‰**

- ç›®å‰æ¨¡å‹è¿è¡Œæ‰€åœ¨çš„ `device` ç”± `ModelLoader.device` å†³å®š
- `SimpleEngine` å†…éƒ¨ä¹Ÿæ˜¯ç›´æ¥ä» `model_loader.device` å– `device`
- è¿™ç§è®¾è®¡è€¦åˆåº¦åé«˜ï¼š  
  - è®¾å¤‡é€»è¾‘ç»‘åœ¨ `ModelLoader` ä¸Šå¹¶ä¸ç†æƒ³  
  - è®¡åˆ’åœ¨ä¸‹ä¸ªç‰ˆæœ¬é‡æ„ device ç®¡ç†ï¼ˆä» Engine æˆ–æ›´ä¸Šå±‚ç»Ÿä¸€ä¸‹å‘ï¼‰

---

**5. æ¨ç†æµç¨‹ & KV Cache è¡Œä¸º**

- **è¾“å…¥å½¢çŠ¶å¤„ç†**
  - æ¨¡å‹éœ€è¦ batch ç»´åº¦çš„è¾“å…¥
  - å•ä¸ª prompt çš„ token ids éœ€è¦ä» `(seq_len,)` è½¬æˆ `(1, seq_len)`

- **Prefill é˜¶æ®µ**
  - è°ƒç”¨ï¼š`model(input_ids, use_cache=True)`
  - è¾“å…¥ï¼šå®Œæ•´çš„ prompt token idsï¼ˆå¸¦ batch ç»´åº¦ï¼‰
  - è¾“å‡ºï¼š
    - `logits`
    - `past_key_values`ï¼ˆKV cacheï¼Œç”¨äºåç»­è§£ç ï¼‰

- **Decode å¾ªç¯é˜¶æ®µ**
  - æ¯ä¸€æ­¥ï¼š
    1. å–æœ€æ–°ç”Ÿæˆçš„ **å•ä¸ª token id**ï¼Œè½¬æˆ tensorï¼ˆå¸¦ batch & seq ç»´åº¦ï¼Œå¦‚ `(1, 1)`ï¼‰
    2. è¿åŒä¸Šä¸€è½®çš„ `past_key_values` ä¸€èµ·é€å…¥æ¨¡å‹
    3. ä½¿ç”¨æ–°è¿”å›çš„ `past_key_values` è¦†ç›–æ—§å€¼
  - æ¨¡å‹åœ¨å†…éƒ¨å¯¹ **prefill** å’Œ **decode** æœ‰ä¸åŒçš„è®¡ç®—è·¯å¾„ï¼š
    - Prefillï¼šä¸€æ¬¡æ€§å¤„ç†å®Œæ•´åºåˆ—ï¼Œå»ºç«‹å®Œæ•´ KV cache
    - Decodeï¼šæ¯æ¬¡åªå¤„ç†ä¸€ä¸ªæ–° tokenï¼Œå¤ç”¨å†å² KV cache

- **è¾“å‡ºè¿˜åŸ**
  - æ”¶é›†æ‰€æœ‰ç”Ÿæˆçš„ `token_ids`
  - ä½¿ç”¨ `tokenizer.decode(...)` è½¬å›æ–‡æœ¬

---

**6. å½“å‰ç‰ˆæœ¬çš„é™åˆ¶**

- åªæ”¯æŒï¼š
  - å•ä¸ª prompt
  - ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ª token çš„å¾ªç¯è§£ç 
- Device é€»è¾‘æš‚æ—¶è€¦åˆåœ¨ `ModelLoader`ï¼Œè®¡åˆ’åœ¨åç»­ç‰ˆæœ¬é‡æ„

# 2025-12-12 

1. åŸæœ¬çš„ minimal generateéƒ¨åˆ†çš„ä»£ç é€»è¾‘ è½¬ç§»åˆ°äº†runtime/api.py
   æŠŠå‚æ•°éƒ¨åˆ† æäº†å‡ºæ¥ 
   1. æ¨¡å‹åå­—        str
   2. promptæœ¬èº«æ˜¯çœŸä¹ˆ str
   3. max new token æäº†å‡ºæ¥
   4. æŠŠè®¾å¤‡åœ¨å“ªé‡Œ æ”¾ç€é‡Œäº†ï¼ˆä½†æ˜¯ç›®å‰è¿˜æ˜¯è€¦åˆçš„ é»˜è®¤è®¾ç½®åœ¨model_loaderå¤„ï¼‰
    æ˜¯å¦åº”è¯¥è®¾è®¡ä¸€ä¸‹ å…ˆä½¿ç”¨assert ä½¿å¾—loaderå’Œengineæ˜¯åœ¨ä¸€ä¸ªdevice cudaï¼š1ä¸Šçš„

  æœ¬èº«çš„minimal_generate.py åŠ å…¥paser
  paser æ¶µç›–ä¸Šè¿°å››ä¸ªç‚¹

  paser å«æœ‰ä¸‰æ­¥
```python
    #1.åˆ›å»ºä¸€ä¸ªparser
    parser = argparse.ArgumentParser()
    
    #2.  ä¸€å †è¦åŠ è¿›å»ç»™argsçš„
    parser.add_argument("--model-name",type = str, required=True, help = "Key in configs/model_paths.yaml")
    parser.add_argument("--prompt", type=str, default="ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±") 

    #3. args = 
    args = parser.parse_args()  

```
æ³¨æ„ argä¸­é—´ - è¿æ¥çš„ åé¢éƒ½è¦å˜æˆ _
```python
    output = generate(
        model_name=args.model_name,
        prompt=args.prompt,
        max_new_tokens=args.max_new_tokens,
        device=args.device,
    )
```
   æ¶‰åŠæ”¹åŠ¨ 
```bash
  [CHANGED]   minimal_generate.py  
  [NEW]       runtime/api.py 
```
    
2. ä½¿ç”¨pytest
    ## è„šæœ¬ & æµ‹è¯• &æ ·ä¾‹ åˆ†ç¦»
    
    #### debug å’Œ test å’Œ exampleçš„åŒºåˆ«   
    ##### 1ï¸âƒ£ Debug â€”â€”ã€Œæˆ‘åœ¨ç†è§£ç³»ç»Ÿã€
    æ ¸å¿ƒç›®çš„  

    ğŸ‘‰ å›ç­”ï¼šâ€œè¿™é‡Œåˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿâ€  
          å…¸å‹ç‰¹å¾  
          æœ‰å¤§é‡ print  
          ä¼šçœ‹ shape / dtype / module  
          è·‘å®Œä¸€æ¬¡å°±å¯èƒ½åˆ   
          å…è®¸ hardcode  
          ä¸ç¨³å®šã€ä¸ä¿è¯é•¿æœŸæˆç«‹  


    ##### 2ï¸âƒ£ Test â€”â€”ã€Œæˆ‘åœ¨ä¿æŠ¤ç³»ç»Ÿã€
        æ ¸å¿ƒç›®çš„
    ğŸ‘‰ å›ç­”ï¼šâ€œè¿™ä¸ªæ¥å£æœ‰æ²¡æœ‰è¢«ç ´åï¼Ÿâ€  

      å…¸å‹ç‰¹å¾  
          1. å‡ ä¹åªæœ‰ assert  
          2. ä¸é äººçœ¼  
          3. å¿«ã€ç¨³å®šã€å¯é‡å¤   
          4. æ˜ç¡® contractï¼ˆshape / type / è¯­ä¹‰ï¼‰  

      å¤±è´¥å«ä¹‰ï¼š  

      âŒ â€œæœ‰ bug äº†ï¼Œå¿…é¡»ä¿®â€

      å…¸å‹é—®é¢˜    
          1. encode æ˜¯å¦å§‹ç»ˆè¿”å› 1D tensorï¼Ÿ  
          2. config_loader æ˜¯å¦æ€»è¿”å› stringï¼Ÿ  
          3. SimpleEngine æ˜¯å¦èƒ½è¢«æ­£ç¡®åˆå§‹åŒ–ï¼Ÿ  
          4. batch/shape contract æœ‰æ²¡æœ‰è¢«ç ´åï¼Ÿ

      ##### 3ï¸âƒ£ Example â€”â€”ã€Œæˆ‘åœ¨å±•ç¤ºç³»ç»Ÿèƒ½åŠ›ã€

æ ¸å¿ƒç›®çš„

  ğŸ‘‰ å›ç­”ï¼šâ€œè¿™ä¸ªç³»ç»Ÿèƒ½å¹²ä»€ä¹ˆï¼Ÿâ€  

å…¸å‹ç‰¹å¾ï¼š  
    1. è·‘å¾—é€šæœ€é‡è¦    
    2. è¾“å‡ºç»™äººçœ‹ï¼ˆæ–‡æœ¬ã€logitsã€é€Ÿåº¦ï¼‰  
    3. å¯æ…¢ã€å¯ä¾èµ– GPU / å¤§æ¨¡å‹   
    4. ç±»ä¼¼ demo / README é‡Œçš„å‘½ä»¤
å¤±è´¥å«ä¹‰ï¼š

âŒ â€œç”¨æˆ·ä½“éªŒåäº† / ç¤ºä¾‹è¿‡æ—¶äº†â€

å…¸å‹é—®é¢˜ï¼š  
    1. èƒ½ä¸èƒ½ç”Ÿæˆä¸€å¥å®Œæ•´ä¸­æ–‡ï¼Ÿ  
    2. å¤šè½® prompt æ•ˆæœå¦‚ä½•ï¼Ÿ  
    3. batch generation æ€ä¹ˆç”¨ï¼Ÿ  

| ç»´åº¦        | Debug | Test      | Example |
| --------- | ----- | --------- | ------- |
| é¢å‘å¯¹è±¡      | è‡ªå·±    | æœªæ¥è‡ªå·± / CI | ç”¨æˆ·      |
| æ˜¯å¦æ¢ç´¢      | âœ…     | âŒ         | âŒ       |
| æ˜¯å¦ assert | å¯æœ‰å¯æ—   | å¿…é¡»        | å¯æœ‰      |
| æ˜¯å¦ print  | âœ…     | âŒ         | âœ…       |
| æ˜¯å¦ç¨³å®š      | âŒ     | âœ…         | âš ï¸      |
| æ˜¯å¦è¿› CI    | âŒ     | âœ…         | âŒ       |
| æ˜¯å¦ä¾èµ– GPU  | éšæ„    | å°½é‡é¿å…      | å¯ä»¥      |

```bash
tests/
â”œâ”€â”€ test_xxxxxx.py     # åªæ”¾ test_xxx 
debug/
â”œâ”€â”€ debug_xxxxxxr.py    # ç”¨æ¥ print / æ‰‹åŠ¨è·‘ debug ç”¨äºæ¢ç´¢
```
 æŠŠdebugæ”¹æˆtestä¸­æ˜¯è¿™æ ·å­çš„
  1. å°†åŸæ¥çš„ 
  ```python
  def main():
  ```

  è½¬æ¢æˆ

    ```python
    def test_tokenizer_encode_decode():
    ```
  æ‰èƒ½ä½¿ç”¨ pytest -q tests/test_tokenizer.py
## test è®¾è®¡çš„ 4 ä¸ªå±‚çº§ï¼ˆä»é‡Œåˆ°å¤–ï¼‰
[1] ç±»å‹ & shape contract  
[2] è¯­ä¹‰ contract  
[3] æ¨¡å—åä½œ contract  
[4] æå°‘é‡ç«¯åˆ°ç«¯ sanity  


### â‘  ç±»å‹ & Shape Contractï¼ˆæœ€ä¼˜å…ˆï¼Œæœ€ç¨³å®šï¼‰

è¿™æ˜¯ æ¨ç†å¼•æ“é¡¹ç›®é‡Œ ROI æœ€é«˜çš„ testã€‚
é€‚åˆ test çš„é—®é¢˜

- encode è¾“å‡ºæ˜¯ä¸æ˜¯ 1D / 2D

- logits shape æ˜¯å¦å›ºå®š

- batch ç»´åº¦æœ‰æ²¡æœ‰å·å·å‡ºç°
  
- KV cache index æœ‰æ²¡æœ‰è¶Šç•Œ

- dtype æ˜¯å¦ä¸º long / float16

ç¤ºä¾‹ï¼ˆTokenizerï¼‰
```python
def test_tokenizer_encode_shape():
    token_ids = tokenizer.encode("hi")
    assert token_ids.ndim == 1
```

ğŸ‘‰ åŸå› ï¼š
shape ä¸€æ—¦å˜äº†ï¼Œæ•´ä¸ª engine éƒ½ä¼š silent break

### â‘¡ è¯­ä¹‰ Contractï¼ˆâ€œä¸ä¼šå˜çš„è¯­ä¹‰â€ï¼‰

ä¸æ˜¯â€œæ¨¡å‹å¥½ä¸å¥½â€ï¼Œè€Œæ˜¯é€»è¾‘å¯¹ä¸å¯¹ã€‚

é€‚åˆ test çš„é—®é¢˜

- decode(encode(x)) â‰ˆ x

- max_new_tokens æ˜¯å¦çœŸçš„é™åˆ¶è¾“å‡º

- ç©º prompt æ˜¯å¦è¢«æ‹’ç» / æ­£ç¡®å¤„ç†

- ä¸åˆæ³•è¾“å…¥æ˜¯å¦æŠ›å¼‚å¸¸

ç¤ºä¾‹
```python 
def test_max_new_tokens_respected():
    out = engine.generate("hi")
    assert len(out) <= expected_upper_bound
```
### â‘¢ æ¨¡å—åä½œ Contractï¼ˆåªæµ‹è¾¹ç•Œï¼Œä¸æµ‹ç»†èŠ‚ï¼‰

è¿™é‡Œéå¸¸å®¹æ˜“ over-testï¼Œè¦å…‹åˆ¶ã€‚

æ­£ç¡®æµ‹æ³•

- engine æ˜¯å¦è°ƒç”¨ tokenizer.encode

- engine æ˜¯å¦ä½¿ç”¨ loader.device

- engine æ˜¯å¦è¿”å› string

é”™è¯¯æµ‹æ³•ï¼ˆä¸è¦ï¼‰

- æ¯ä¸€å±‚ transformer æ˜¯å¦è¢«è°ƒç”¨

- logits æ•°å€¼æ˜¯å¤šå°‘

### â‘£ End-to-End Sanityï¼ˆæœ€å¤š 1ï½2 ä¸ªï¼‰

ä¸æ˜¯ accuracy testï¼Œåªæ˜¯â€œè¿˜æ´»ç€å—â€

ç¤ºä¾‹
@pytest.mark.gpu
def test_engine_can_generate_one_token():
    out = engine.generate("ä½ å¥½")
    assert isinstance(out, str)


âš ï¸ åªè¦ 1 ä¸ªå°±å¤Ÿäº†

## ã€Œæ¨¡æ¿ã€è®¾è®¡ä¸€ä¸ª test

ä»¥åä½ ç»™ä»»ä½•æ¨¡å—å†™ testï¼Œå¥—è¿™ä¸ªæ¨¡æ¿å°±è¡Œã€‚

### Step 1ï¼šå†™ä¸‹ contractï¼ˆè‹±æ–‡/ä¸­æ–‡éƒ½è¡Œï¼‰

- Tokenizer.encode:

- input: str

- output: 1D torch.LongTensor

- no batch dim

### Step 2ï¼šæŠŠ contract ç¿»æˆ assert
```python
assert isinstance(token_ids, torch.Tensor)
assert token_ids.ndim == 1
assert token_ids.dtype == torch.long
```

### Step 3ï¼šåˆ æ‰æ‰€æœ‰ print

å¦‚æœä½ å‘ç°ï¼š

â€œä¸ print æˆ‘ä¸çŸ¥é“å¯¹ä¸å¯¹â€

ğŸ‘‰ é‚£è¯´æ˜ å®ƒè¿˜ä¸æ˜¯ testï¼Œå›å»å†™ debug

å››ã€ä½ å½“å‰é¡¹ç›®ã€Œç«‹åˆ»å€¼å¾—å†™ testã€çš„æ¸…å•ï¼ˆå¾ˆå…·ä½“ï¼‰  
âœ… å¿…å†™ï¼ˆç°åœ¨å°±è¯¥æœ‰ï¼‰

- Tokenizer encode/decode contract

- Config loader è¿”å›ç±»å‹

- SimpleEngine åˆå§‹åŒ– & å‚æ•°é€ä¼ 

- shape / batch ä¸å˜é‡

âš ï¸ é€‰å†™ï¼ˆä¸‹é˜¶æ®µï¼‰

- å• step generation shape

- KV cache index contract

- scheduler è¾“å…¥è¾“å‡º shape

âŒ ä¸å†™ï¼ˆæˆ–æå°‘ï¼‰

- æ¨¡å‹æ•°å€¼æ­£ç¡®æ€§
- æ–‡æœ¬ç”Ÿæˆè´¨é‡
- æ€§èƒ½

# 2025-12-13 

ä»Šå¤©å†™äº†request ç›®çš„æ˜¯ä¸ºäº†å¯¹æ ‡vllm
ä¸ºä»€ä¹ˆéœ€è¦request
çœ‹è¿‡è°ƒåº¦ç­–ç•¥ä¹‹åå°±ä¼šæ˜ç™½ ä¸å¯èƒ½ä¸€ä¸ªè´ªå¿ƒç®—æ³•ä¸€ç›´åªç®—ä¸€ä¸ªrequest
æ‰€ä»¥éœ€è¦å®šä¹‰ä¸€ä¸ªæ•°æ®ç±» é€‰ç”¨dataclass
é™¤äº†è‡ªèº«idä¹‹å¤–
è¿˜éœ€è¦æœ‰è¿™ä¸ªpromptæ˜¯ä»€ä¹ˆ
å…¶æ¬¡è¦æœ‰ä¸€ä¸ªmax_new_tokenæ¥é™åˆ¶ç”Ÿæˆ è¿™ä¸ªåº”è¯¥æ˜¯å¯¹æ ‡äº†ç›®å‰çš„engine ä¸­çš„generate
å› ä¸ºä¸€æ¬¡éœ€è¦ç”Ÿæˆå¤šå°‘ä¸ªtokenæ˜¯å®šä¹‰åœ¨è¿™ä¸€ä¸ªè¯·æ±‚é‡Œçš„
æ‰€ä»¥åˆ†å¼€æ”¾åœ¨requesté‡Œ ä½œä¸ºä¸€ä¸ªçŠ¶æ€æœº

é™¤äº†promptæœ¬èº«
1. æ”¶åˆ° prompt
2. tokenize å¾—åˆ° input_ids
3. é€ token decodeã€ä¸æ–­ append æ–° token
4. è¾¾åˆ° max_new_tokens / eos ç»“æŸ
5. æ ‡è®° finishedï¼Œé‡Šæ”¾ cache



 






























